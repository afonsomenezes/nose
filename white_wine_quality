#Importing the data
import pandas as pd 
path = 'C:\\Users\\(...)\\wine_white.csv'
df = pd.read_csv(path, sep = ';') 
df.head() #to get a overview of the data

#Target variable is 'quality' - let's check for the possible values
from collections import Counter
Counter(df['quality']) #values from 3 to 9 (the higher the better) and pretty unbalanced classes (6 and 5 quality wines make up to almost 75% of the dataset)
#let's get a visual representation of the data dispersion
import matplotlib.pyplot as plt
import seaborn as sns
sns.countplot(x = 'quality', data = df)
plt.show() #visual representation that confirms the previously seen distribution
df.dtypes #floats and integers
df.describe()
df['pH'].plot(kind = 'hist', bins = 100) #plotting graphs to check the variables
plt.xlabel('pH')
plt.show()

#Missing values
df.isnull().any() #the output is True/False for each column, indicating the presence (absence) of missings with True (False)
#in this case we saw that there are no missing values
df.isnull().sum() #would sum the number of missing valuer per column (0 in our case)

#Outliers
#will analyze each variable individually to check for the presence of outliers
df.describe() #gives us a clear overview of the values we will encounter
plt.boxplot(df['fixed acidity']) #will remove all the values above 10
plt.show() #repeat to visualize each boxplot
df = df[df['fixed acidity'] <= 10]
plt.boxplot(df['volatile acidity']) #considering that all the values range from 0 to 1, I will not remove the outliers
plt.boxplot(df['citric acid']) #values from 0 to 1.5, won't remove any value either
plt.boxplot(df['residual sugar'])
df = df[df['residual sugar'] <= 30]
plt.boxplot(df['chlorides']) #values ranging from o to 0.35
plt.boxplot(df['free sulfur dioxide'])
df = df[df['free sulfur dioxide'] <= 125]
plt.boxplot(df['total sulfur dioxide'])
df = df[df['total sulfur dioxide'] <= 275]
plt.boxplot(df['density'])
plt.boxplot(df['pH'])
plt.boxplot(df['sulphates'])
plt.boxplot(df['alcohol'])
df.describe()

#Correlation for feature selection
df.corr(method = 'pearson')
plt.figure(figsize=(12,10))
cor = df.corr(method = 'pearson')
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()
#despite being a small dataset, density and residual sugar are pretty correlated 0.83 - will leave density because it is the one with less outliers and is more correlated with the target variable
df = df.drop(columns = 'residual sugar')

#separating the independent and the dependent variables
x = df.iloc[:, 0:10]
y = df.iloc[:, -1]

#standardizing the variables
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_std = sc.fit_transform(x)

#splitting into train and test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3) #non standardized variables
x_std_train, x_std_test, y_train1, y_test1 = train_test_split(x_std, y, test_size = 0.3) #non standardized variables

#KNN - 7
from sklearn.neighbors import KNeighborsClassifier
y.unique()
knn = KNeighborsClassifier(n_neighbors = 7)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred) #0.5 accuracy - quite a low value

#Decision Tree
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(x_train,y_train)
dt_predict = dt.predict(x_test)
from sklearn.metrics import confusion_matrix
accuracy_score(y_test, dt_predict) #0.6 accuracy

#Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf_predict=rf.predict(x_test)
accuracy_score(y_test, rf_predict) #0.65 accuracy

#now with the standardized data
#KNN - 7
knn.fit(x_std_train, y_train1)
y_pred_std = knn.predict(x_std_test)
accuracy_score(y_test1, y_pred_std) #0.54 accuracy - better

#Decision Tree
dt.fit(x_std_train,y_train1)
dt_predict = dt.predict(x_std_test)
accuracy_score(y_test1, dt_predict) #0.6 accuracy - lower

#Random Forest
rf.fit(x_std_train, y_train1)
rf_predict=rf.predict(x_std_test)
accuracy_score(y_test1, rf_predict) #0.66 accuracy - slightly better

#preprocessing the standardized data frame
df_std = pd.DataFrame(x_std) #code to transform the array of the standardized variables to a data frame
df_std.columns = list(x) #rename the new data frame columns

#PCA - apply it to the standardized variables
from sklearn.decomposition import PCA
pca = PCA()
x_std_pca = pca.fit_transform(x_std)
#plotting the PCA graph
explained_variance = np.var(x_pca, axis=0)
explained_variance_ratio = explained_variance / np.sum(explained_variance)
np.cumsum(explained_variance_ratio)
plt.figure(figsize=(10,10))
plt.plot(np.cumsum(explained_variance_ratio), 'ro-')
plt.grid()
#the first principal components do not explain much of the variance, therefore I will move on with all the variables

#analyze variables - RF standardized
#next: quality variable into 0 or 1 (bad or good) - new division to see if it outperforms the previous one
